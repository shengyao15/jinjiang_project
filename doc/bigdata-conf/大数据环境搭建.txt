
修改host hostname-------------------------------------
vi /etc/hosts
hostname hadoop1
vi /etc/sysconfig/network

修改profile------------------------------
export JAVA_HOME=/usr/java/jdk1.6.0_45
export PATH=$PATH:$JAVA_HOME/bin:$JAVA_HOME/jre/bin
export CLASSPATH=.:$JAVA_HOME/lib:$JAVA_HOME/jre/lib:$CLASSPATH

export PATH=$PATH:/usr/hadoop/hadoop-2.2.0/bin
export PATH=$PATH:/usr/hadoop/hadoop-2.2.0/sbin

/etc/init.d/iptables stop

export SPARK_HOME=/usr/spark/spark-1.0.2-bin-hadoop2
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin

export PATH=$PATH:/usr/hbase/hbase-0.96.2-hadoop2/bin



免密码登录----------------------------------
ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys

soft link-------------------------
cd ~
ln -s /usr/hadoop/hadoop-2.2.0 hadoop
ln -s /usr/hbase/hbase-0.96.2-hadoop2 hbase
ln -s /usr/spark/spark-1.0.2-bin-hadoop2 spark


标签----------------------

hdfs
http://hadoop1:50070/dfshealth.jsp

yarn
http://hadoop1:8088/cluster/cluster

spark
http://hadoop1:8080/

hbase
http://hadoop1:60010/


创建目录---------------------------------
mkdir -p /data/hadoop/dfs/name
mkdir -p /data/hadoop/dfs/data
mkdir -p /data/hadoop/tmp


运行测试hadoop -----------------------------
hadoop namenode -format
start-dfs.sh
start-yarn.sh

hdfs dfs -mkdir /in
hdfs dfs -ls /

mkdir /in
cp /root/hadoop/README.txt /in
hadoop dfs -copyFromLocal /root/hadoop/README.txt /in
hadoop jar /root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.2.0.jar wordcount /in /out
hadoop jar /root/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.2.0.jar pi 2 2 

------------------------------
HDFS配置
hadoop-env.sh
core-site.xml
hdfs-site.xml

map-reduce配置
mapred-env.sh
mapred-site.xml

yarn配置
yarn-site.xml

slaves

spark配置---------------------------------
spark-env.sh
slaves

spark shell 重命名
mv /root/spark/sbin/start-all.sh /root/spark/sbin/start-spark-all.sh
mv /root/spark/sbin/stop-all.sh /root/spark/sbin/stop-spark-all.sh

spark测试--------------------------------------

spark-shell

val lines = sc.textFile("/root/spark/README.md")   
lines.count()   
lines.first()   
val pythonLines = lines.filter(line => line.contains("Python"))   
lines.first()  


命令对于的进程----------------------------------
start-dfs.sh
NameNode
DataNode
SecondaryNameNode

start-yarn.sh
NodeManager
ResourceManager

start-spark-all.sh
Master
Worker


--------------------------
和yarn集成  -- 一定要启动hadoop了
参照 http://my.oschina.net/u/1169079/blog/292435
export HADOOP_HOME=/usr/hadoop/hadoop-2.2.0
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
SPARK_EXECUTOR_INSTANCES=1
SPARK_EXECUTOR_CORES=1
SPARK_EXECUTOR_MEMORY=1g
SPARK_DRIVER_MEMORY=1g
SPARK_YARN_APP_NAME="TristanSpark"

测试
val lines = sc.textFile("/in/README.txt").count 

hbase 配置---------------------------------
hbase-env.sh
hbase-site.xml
regionservers

mkdir -p /data/hbase/zookeeper

hbase 测试-------------------------------------
start-hbase.sh
hbase shell
create 'mytable', 'cf' 
put 'mytable','first','cf:info','patience and courage' 
list 
scan "mytable" 
get "mytable","first" 
count "mytable" 

hbase--------------------------

http://www.abcn.net/2014/07/lighting-spark-with-hbase-full-edition.html

spark-submit --master spark://hadoop1:7077 --class spark.examples.SparkWordCount2 --name SparkWordCount SparkWordCount.jar
spark-submit --class spark.examples.SparkWordCount2 --name SparkWordCount SparkWordCount.jar


spark-submit --driver-class-path "/usr/hbase/hbase-0.96.2-hadoop2/lib/*"  --class spark.examples.Tristan02 --name Tristan02 SparkWordCount.jar

spark-shell --driver-class-path "/usr/hbase/hbase-0.96.2-hadoop2/lib/*"


------------------------------------------------
处理hbase的问题
hadoop fs -rmr /hbase
2015-03-13 18:34:40,856 ERROR [MASTER_TABLE_OPERATIONS-hadoop1:45125-0] handler.CreateTableHandler: Error trying to create the table hbase:namespace
java.io.IOException: Unable to move table from temp=hdfs://hadoop1:9000/hbase/.tmp/data/hbase/namespace to hbase root=hdfs://hadoop1:9000/hbase/data/hbase/namespace

修改个路径
hbase.rootdir
hbase.zookeeper.property.dataDir

spark-submit
java.lang.IllegalStateException: unread block data

无法解决
直接用hp上的虚拟机了







---------------------
dbp 阅读
1> bigdata-web  
智能推荐系统管理平台
表
tb_rec_scene
tb_rec_scene_config


2> katherine
SendEdmMain
TB_REC_CH_EDM_PROMOTION_MEMENBER_HOTEL
3天之内
查找邮件地址   T_DBP_MEMBER_MERGE
T_DBP_EDM_SEND_LOG

3> dbp
LoginDataResource
DataTaskResource
DmsRecommendResource
MobileLocationResource
MockRecommendResource
RecommendResource


----------------------------------------
视频
MLLIB------------
kmeans ok

ALS
java.lang.StackOverflowError java.util.zip.Inflater.inflate

增加参数(特别是-Xss10m)
SPARK_EXECUTOR_INSTANCES=1
SPARK_EXECUTOR_CORES=1
SPARK_EXECUTOR_MEMORY=2g
SPARK_DRIVER_MEMORY=2g
SPARK_JAVA_OPTS=-Xss10m
参照
http://f.dataguru.cn/thread-368353-1-1.html
http://mail-archives.us.apache.org/mod_mbox/spark-user/201403.mbox/%3CCABeDAjGkqK9LLNkEw9P=_QAAk5+GJp2H7897srO9=0DXc9VFPA@mail.gmail.com%3E
官方例子
http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html


运行很长时间后
RMSE (validation) = 0.8836263949566461 for the model trained with rank = 12, lambda = 10.0, and numIter = 20.

如果不改参数，修改下val numIters = List(10, 20)的值就ok了 
--> 锦江是用什么参数的？

streaming-------------------------

dstream ( discrete stream 离散) 是rdd按时间排序的集合

三种场景
无状态/有状态stateful/window

输入源 需要手动加载jar包

主要方法
transform  可以和历史数据的rdd做join等操作， 在线的机器学习
updateStateByKey  累加器
window操作 reduceByKeyAndWindow  

持久化
会有个备份，流数据不能重现
checkpoint 写到硬盘
对于stateful和window必须checkpoint

RDD的某些partition丢失，可以通过lineage重新计算得到


