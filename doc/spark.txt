./bin/spark-shell

//spark practise
val lines = sc.textFile("README.md")   
lines.count()   
lines.first()
lines.take(10)
lines.foreach(println) 
lines.collect()
val pythonLines = lines.filter(line => line.contains("Python"))   
pythonLines.first()  


val file = sc.textFile("hdfs://hadoop1:9000/in/hdfs-site.xml")
val count = file.flatMap(line => line.split(" ")).map(word => (word, 1)).reduceByKey(_+_)
count.collect()



./bin/spark-submit --master spark://hadoop1:7077 --class org.apache.spark.examples.SparkPi --name Spark-Pi /usr/spark/spark-1.0.2-bin-hadoop2/lib/spark-examples-1.0.2-hadoop2.2.0.jar

./bin/spark-submit --master yarn --class org.apache.spark.examples.SparkPi --name Spark-Pi /usr/spark/spark-1.0.2-bin-hadoop2/lib/spark-examples-1.0.2-hadoop2.2.0.jar

------------------------------------------------------------------

import org.apache.spark._
import SparkContext._
val sc = new SparkContext(args[0],"WordCount",System.getenv("SPARK_HOME"),Seq(System.getenv("SPARK_TEST_JAR")))
val textRDD = sc.textFile(args[1])

-----------------------------------------------------------
hadoop dfs -copyFromLocal README.md /in

./bin/spark-submit --name SparkWordCount --class spark.examples.SparkWordCount --master spark://hadoop1:7077 --executor-memory 512M --total-executor-cores 1 ./bin/SparkWordCount.jar /in/README.md

---------

export SPARK_JAVA_OPTS="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=9999"




